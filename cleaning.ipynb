{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb73ded-c374-4fc8-a750-1d02246e5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import emot\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23815af2-bffc-4ee4-91a0-60f803b8ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbc9cb-c6d1-4484-906c-50f9fe0d751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf2108-490e-4a16-bce2-b946304a304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train_gr/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17164706-b4ab-42d3-a733-98155002a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column(col):\n",
    "    '''\n",
    "    input --> column as series\n",
    "    clean from extra space characters, digits, links\n",
    "    return Series: the clean column\n",
    "    '''\n",
    "    rev = col.copy()\n",
    "    rev = (rev.str.replace(r'\\n|\\r|\\t', ' ', regex=True)\n",
    "           .str.replace(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', ' ', regex=True)\n",
    "           .str.replace(r'\\s+', ' ', regex=True)\n",
    "           .str.lower()\n",
    "          )\n",
    "    \n",
    "    return rev\n",
    "\n",
    "def tokenize_clean(col, stem=True, lemmatize=False):\n",
    "    '''\n",
    "    tokenize column and remove stopwords and punctuation\n",
    "    apply stemmer of lemmatizer\n",
    "    return tokens as a list of lists\n",
    "    '''\n",
    "    rev = col.copy()\n",
    "    tokens = rev.apply(word_tokenize)\n",
    "    # remove stopwords\n",
    "    words = stopwords.words(\"english\")\n",
    "    lambd = lambda s: [word for word in s if word not in words]\n",
    "    tokens = tokens.apply(lambd)\n",
    "    # remove punctuation\n",
    "    import string\n",
    "    punk = list(string.punctuation)\n",
    "    lambd = lambda s: [word for word in s if word not in punk + ['...']]\n",
    "    tokens = tokens.apply(lambd)\n",
    "    \n",
    "    if stem:\n",
    "        tokens = tokens.apply(lambda s: [SnowballStemmer(\"english\").stem(w) for w in s])\n",
    "                              \n",
    "    return tokens\n",
    "    \n",
    "def find_emoji(df, column):\n",
    "        #no multiples\n",
    "        #keep only meaning \n",
    "        #df['emoji']=df[column].apply(emot_obj.emoji)\n",
    "        #df['emoticon']=df[column].apply(emot_obj.emoticons)\n",
    "        x={}\n",
    "        emot_obj = emot.core.emot()\n",
    "        for index,row in df.iterrows():\n",
    "            ej=set(emot_obj.emoji(row[column])['mean'])\n",
    "            et=set(emot_obj.emoticons(row[column])['mean'])\n",
    "            x[row['review_id']]=(list(ej)+list(et))\n",
    "        emojies = pd.Series(x)\n",
    "        return pd.DataFrame(emojies).reset_index().rename({'index': 'review_id', 0: 'emoticon'}, axis=1)\n",
    "    \n",
    "def preprocess_dataframe(path='../data/train_gr/train.csv'):\n",
    "    '''\n",
    "    Load dataframe from path\n",
    "    Add columns for clean, tokenized and emojies\n",
    "    return the initial dataframe with extra columns\n",
    "    '''\n",
    "    df = pd.read_csv(path)\n",
    "    df['clean'] = clean_column(df.user_review)\n",
    "    df['tokens'] = tokenize_clean(df.clean)\n",
    "    \n",
    "    # find emojies and merge them into dataframe\n",
    "    emojies = find_emoji(df, 'user_review')\n",
    "    df = df.merge(emojies, on='review_id', how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85f23a-bb4a-43e2-acd4-0d2b290220fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = preprocess_dataframe('../data/train_gr/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a761731-9028-4108-b9c8-a4a4c534e7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a835c-9010-478f-9411-ff75f269b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('../data/train_gr/train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81b5b2-75ba-454c-9b18-677bb8fc01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojies = find_emoji(df, 'user_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72dbb67-13e1-4750-a780-653523266cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
